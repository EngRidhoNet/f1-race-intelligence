# .env.example
# Application
APP_NAME="F1 Race Intelligence Backend"
APP_VERSION="1.0.0"
DEBUG=false

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/f1_intelligence

# LLM Configuration
# Options: "ollama", "openai_compatible"
LLM_PROVIDER=ollama

# For Ollama (local models)
LLM_API_BASE_URL=http://localhost:11434
LLM_MODEL_NAME=llama3

# For OpenAI-compatible APIs (vLLM, OpenRouter, etc.)
# LLM_PROVIDER=openai_compatible
# LLM_API_BASE_URL=https://api.openrouter.ai/v1
# LLM_MODEL_NAME=meta-llama/llama-3-70b-instruct
# LLM_API_KEY=your_api_key_here

LLM_TIMEOUT=60

# WebSocket
REPLAY_FPS=10

# FastF1 Cache
FASTF1_CACHE_DIR=./fastf1_cache

# CORS
CORS_ORIGINS=["http://localhost:3000","http://localhost:5173"]

# ---

# .dockerignore
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
.env
.env.*
fastf1_cache/
*.db
.pytest_cache/
.coverage
htmlcov/
.mypy_cache/
.DS_Store

# ---

# docker-compose.yml
version: '3.8'

services:
  db:
    image: postgres:15
    environment:
      POSTGRES_USER: f1user
      POSTGRES_PASSWORD: f1password
      POSTGRES_DB: f1_intelligence
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U f1user"]
      interval: 10s
      timeout: 5s
      retries: 5

  backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://f1user:f1password@db:5432/f1_intelligence
      LLM_PROVIDER: ollama
      LLM_API_BASE_URL: http://host.docker.internal:11434
      LLM_MODEL_NAME: llama3
      FASTF1_CACHE_DIR: /app/fastf1_cache
    volumes:
      - ./fastf1_cache:/app/fastf1_cache
    depends_on:
      db:
        condition: service_healthy
    command: >
      sh -c "
        alembic upgrade head &&
        uvicorn app.main:app --host 0.0.0.0 --port 8000
      "

volumes:
  postgres_data: