# .env.example
# Application
APP_NAME="F1 Race Intelligence Backend"
APP_VERSION="1.0.0"
DEBUG=false

# Database
DATABASE_URL=postgresql://localhost:5432/f1_intelligence

# LLM Configuration
# Options: "ollama", "openai_compatible"
LLM_PROVIDER=ollama

# For Ollama (local models)
LLM_API_BASE_URL=http://localhost:11434
LLM_MODEL_NAME=llama3

# For OpenAI-compatible APIs (vLLM, OpenRouter, etc.)
# LLM_PROVIDER=openai_compatible
# LLM_API_BASE_URL=https://api.openrouter.ai/v1
# LLM_MODEL_NAME=meta-llama/llama-3-70b-instruct
# LLM_API_KEY=your_api_key_here

LLM_TIMEOUT=60

# WebSocket
REPLAY_FPS=10

# FastF1 Cache
FASTF1_CACHE_DIR=./fastf1_cache

# CORS
CORS_ORIGINS=["http://localhost:3000","http://localhost:5173"]



